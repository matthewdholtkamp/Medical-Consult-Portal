<!DOCTYPE html>
<html lang="en" class="dark">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dr. H - Clinical Consultant</title>

    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            darkMode: 'class',
            theme: {
                extend: {
                    colors: {
                        gemini: {
                            800: '#1e293b',
                            900: '#0f172a',
                        }
                    },
                    animation: {
                        'pulse-slow': 'pulse 3s cubic-bezier(0.4, 0, 0.6, 1) infinite',
                    }
                }
            }
        }
    </script>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@24,400,0,0" />

    <style>
        body { font-family: 'Inter', sans-serif; background-color: #0f172a; }

        /* Audio Pulse Animation */
        @keyframes pulse-ring {
            0% { transform: scale(0.8); opacity: 0.5; }
            100% { transform: scale(2.2); opacity: 0; }
        }
        
        .pulse-active::before {
            content: '';
            position: absolute;
            left: 0; top: 0; right: 0; bottom: 0;
            border-radius: 50%;
            border: 2px solid #60a5fa;
            animation: pulse-ring 2s cubic-bezier(0.215, 0.61, 0.355, 1) infinite;
        }
        
        .pulse-active::after {
            content: '';
            position: absolute;
            left: 0; top: 0; right: 0; bottom: 0;
            border-radius: 50%;
            border: 2px solid #60a5fa;
            animation: pulse-ring 2s cubic-bezier(0.215, 0.61, 0.355, 1) infinite;
            animation-delay: 0.5s;
        }

        @keyframes think-glow {
            0%, 100% { box-shadow: 0 0 20px rgba(168, 85, 247, 0.2); border-color: rgba(168, 85, 247, 0.5); }
            50% { box-shadow: 0 0 40px rgba(168, 85, 247, 0.6); border-color: rgba(168, 85, 247, 1); }
        }
        .thinking-active {
            animation: think-glow 2s infinite;
        }

        .background-overlay {
            position: fixed;
            top: 0; left: 0; width: 100%; height: 100%;
            z-index: -1;
            background-image: radial-gradient(circle at 2px 2px, rgba(255,255,255,0.05) 1px, transparent 0);
            background-size: 32px 32px;
            opacity: 0.5; 
            pointer-events: none; 
        }

        .audio-bar {
            width: 4px;
            background: #60a5fa;
            border-radius: 2px;
            transition: height 0.1s ease;
        }

        .chat-message {
            opacity: 0;
            animation: fadeIn 0.3s forwards;
        }
        @keyframes fadeIn { to { opacity: 1; } }
        
        /* Custom scrollbar for chat */
        .custom-scroll::-webkit-scrollbar { width: 4px; }
        .custom-scroll::-webkit-scrollbar-track { background: transparent; }
        .custom-scroll::-webkit-scrollbar-thumb { background: #334155; border-radius: 2px; }
        
        /* Dictation Pulse */
        @keyframes dictation-pulse {
            0% { transform: scale(1); opacity: 1; }
            50% { transform: scale(1.2); opacity: 0.8; }
            100% { transform: scale(1); opacity: 1; }
        }
        .dictating {
            animation: dictation-pulse 1.5s infinite;
            color: #f87171 !important; /* Red */
        }
    </style>
</head>

<body class="text-gray-200 h-screen overflow-hidden flex flex-col relative selection:bg-blue-500 selection:text-white">

    <div class="background-overlay"></div>

    <!-- Header -->
    <header class="h-16 flex items-center justify-between px-6 z-20 border-b border-white/5 bg-gemini-900/50 backdrop-blur-sm flex-shrink-0">
        <div class="flex items-center gap-3">
            <div class="w-8 h-8 rounded-lg bg-gradient-to-br from-blue-500 to-indigo-600 flex items-center justify-center shadow-lg shadow-blue-500/20">
                <span class="material-symbols-outlined text-white text-[20px]">medical_services</span>
            </div>
            <div>
                <h1 class="font-bold text-slate-100 text-sm tracking-wide">Dr. Holtkamp</h1>
                <p class="text-[10px] text-slate-500 uppercase tracking-wider font-medium">Expert Clinical Consultant</p>
            </div>
        </div>
        <div id="status-badge" class="flex items-center gap-2 px-3 py-1 rounded-full bg-slate-800 border border-slate-700">
            <div id="status-dot" class="w-2 h-2 rounded-full bg-slate-500"></div>
            <span id="status-text" class="text-xs text-slate-400 font-mono font-bold">STANDBY</span>
        </div>
    </header>

    <!-- Main Content: Flex Column Layout for Stability -->
    <main class="flex-1 flex flex-col overflow-hidden relative z-10">
        
        <!-- Conversation Log: Flex Grow to Fill Space -->
        <div id="conversation-container" class="flex-1 overflow-y-auto px-4 py-6 space-y-4 custom-scroll">
            <div class="text-center text-slate-500 text-xs mt-4">Session Started. Evidence-based medicine (OpenEvidence/UpToDate).</div>
            <!-- Messages injected here -->
        </div>

        <!-- Input Area: Stays at Bottom Naturally -->
        <div class="bg-slate-900/90 backdrop-blur-md border-t border-slate-800 p-4 pb-4 flex-shrink-0 flex flex-col gap-3">
            
            <!-- Audio Controls (Independent) -->
            <div id="audio-controls" class="hidden flex items-center justify-center gap-4 mb-2 p-2 bg-slate-800/50 rounded-full border border-slate-700/50 backdrop-blur-sm transition-all mx-auto w-fit">
                <button onclick="window.playAudio()" class="p-2 rounded-full hover:bg-slate-700 text-green-400 transition-colors" title="Play Audio">
                    <span class="material-symbols-outlined text-2xl">play_circle</span>
                </button>
                <button onclick="window.pauseAudio()" class="p-2 rounded-full hover:bg-slate-700 text-amber-400 transition-colors" title="Pause Audio">
                    <span class="material-symbols-outlined text-2xl">pause_circle</span>
                </button>
            </div>

            <!-- Visualizer Bars (Small) -->
            <div class="flex items-center justify-center gap-1 h-4 opacity-50 transition-opacity duration-300" id="audio-bars-container">
                <div class="audio-bar h-1"></div>
                <div class="audio-bar h-2"></div>
                <div class="audio-bar h-1"></div>
                <div class="audio-bar h-3"></div>
                <div class="audio-bar h-1"></div>
            </div>

            <!-- Input Bar -->
            <div class="max-w-3xl mx-auto w-full flex items-end gap-2 bg-slate-800 p-2 rounded-3xl border border-slate-700 shadow-lg focus-within:ring-2 focus-within:ring-blue-500/50 transition-all">
                
                <!-- Main Voice Mode Button (Conversation) -->
                <button onclick="handleMicToggle()" id="main-mic-btn" class="w-10 h-10 mb-0.5 rounded-full bg-slate-700 hover:bg-slate-600 flex items-center justify-center transition-colors flex-shrink-0 group" title="Hold Conversation">
                    <span class="material-symbols-outlined text-slate-300 group-hover:text-white transition-colors" id="mic-icon">record_voice_over</span>
                </button>

                <!-- Expanding Text Input -->
                <textarea id="text-input" rows="1" placeholder="Type query or tap mic to speak..." class="flex-1 bg-transparent border-none outline-none text-sm text-white px-2 py-2.5 font-medium placeholder:text-slate-500 resize-none overflow-hidden max-h-32" onkeydown="handleInputKey(event)"></textarea>

                <!-- Send Button -->
                <button onclick="handleTextSubmit()" class="w-10 h-10 mb-0.5 rounded-full bg-blue-600 hover:bg-blue-500 flex items-center justify-center transition-colors flex-shrink-0">
                    <span class="material-symbols-outlined text-white text-lg">arrow_upward</span>
                </button>
            </div>
            
            <div class="text-center h-4">
                <span id="interaction-status" class="text-[10px] font-bold tracking-widest text-slate-500 uppercase transition-colors">Ready</span>
            </div>
        </div>

    </main>

    <!-- PCM to WAV Library (Inlined for single file portability) -->
    <script>
    !function(e,t){"object"==typeof exports&&"object"==typeof module?module.exports=t():"function"==typeof define&&define.amd?define([],t):"object"==typeof exports?exports.pcmToWav=t():e.pcmToWav=t()}(window,function(){return function(e){var t={};function r(n){if(t[n])return t[n].exports;var o=t[n]={i:n,l:!1,exports:{}};return e[n].call(o.exports,o,o.exports,r),o.l=!0,o.exports}return r.m=e,r.c=t,r.d=function(e,t,n){r.o(e,t)||Object.defineProperty(e,t,{enumerable:!0,get:n})},r.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},r.t=function(e,t){if(1&t&&(e=r(e)),8&t)return e;if(4&t&&"object"==typeof e&&e&&e.__esModule)return e;var n=Object.create(null);if(r.r(n),Object.defineProperty(n,"default",{enumerable:!0,value:e}),2&t&&"string"!=typeof e)for(var o in e)r.d(n,o,function(t){return e[t]}.bind(null,o));return n},r.n=function(e){var t=e&&e.__esModule?function(){return e.default}:function(){return e};return r.d(t,"a",t),t},r.o=function(e,t){return Object.prototype.hasOwnProperty.call(e,t)},r.p="",r(r.s=0)}([function(e,t,r){"use strict";Object.defineProperty(t,"__esModule",{value:!0}),t.default=function(e,t,r){var n,o,a,i,f,c,u,d=new DataView(new ArrayBuffer(44+e.length*t*2));return d.setUint32(0,1380533830,!1),d.setUint32(4,36+e.length*t*2,!0),d.setUint32(8,1463899717,!1),d.setUint32(12,1718449184,!1),d.setUint32(16,16,!0),d.setUint16(20,1,!0),n=t,d.setUint16(22,n,!0),o=r,d.setUint32(24,o,!0),a=r*t*2,d.setUint32(28,a,!0),i=2*t,d.setUint16(32,i,!0),f=16,d.setUint16(34,f,!0),d.setUint32(36,1684108385,!1),c=e.length*t*2,d.setUint32(40,c,!0),u=new Uint8Array(d.buffer,44),e.forEach(function(e,t){var r=44+2*t;d.setInt16(r,e,!0)}),new Blob([d.buffer],{type:"audio/wav"})},e.exports=t.default}])});
    </script>

    <script type="module">
        import { API_KEYS } from './js/config.js?v=secure';

        // --- Configuration ---
        let API_KEY = API_KEYS.PSYCHIATRY;
        const MODEL_ID = "gemini-2.5-flash"; // Main chat model
        const TTS_MODEL_ID = "gemini-2.5-flash-preview-tts"; // Model for Audio Generation (Aoede)
        
        // --- State Machine ---
        const STATE = {
            IDLE: 'IDLE',
            LISTENING: 'LISTENING',
            THINKING: 'THINKING',
            GENERATING_VOICE: 'GENERATING_VOICE',
            SPEAKING: 'SPEAKING',
            ERROR: 'ERROR'
        };
        let currentState = STATE.IDLE;

        // --- Globals ---
        let recognition = null; // Web Speech API
        let audioContext = null; // Visualizer
        let analyser = null;
        let microphoneStream = null;
        let dataArray = null;
        let animationFrame = null;
        let currentAudio = null; // For native TTS playback
        let conversationHistory = []; 
        let isDictating = false; // Flag to keep mic alive

        // --- UI References ---
        const ui = {
            btn: document.getElementById('main-mic-btn'),
            icon: document.getElementById('mic-icon'),
            statusText: document.getElementById('status-text'),
            statusDot: document.getElementById('status-dot'),
            interactionText: document.getElementById('interaction-status'),
            bars: document.querySelectorAll('.audio-bar'),
            barsContainer: document.getElementById('audio-bars-container'),
            conversation: document.getElementById('conversation-container'),
            textInput: document.getElementById('text-input'),
            audioControls: document.getElementById('audio-controls')
        };

        // --- Auto-Resize Textarea ---
        ui.textInput.addEventListener('input', function() {
            this.style.height = 'auto';
            this.style.height = (this.scrollHeight) + 'px';
            if(this.value === '') this.style.height = 'auto'; // Reset
        });

        // --- Core Logic ---

        // --- Audio Control Logic ---
        window.playAudio = function() {
            if(currentAudio) {
                currentAudio.play();
                updateState(STATE.SPEAKING);
            }
        };

        window.pauseAudio = function() {
            if(currentAudio) {
                currentAudio.pause();
                ui.statusText.innerText = "PAUSED";
                ui.statusDot.className = "w-2 h-2 rounded-full bg-amber-500";
            }
        };

        // --- Mic Logic (Live Transcription) ---
        window.handleMicToggle = function() {
            if (currentState === STATE.IDLE) {
                startDictation();
            } else if (currentState === STATE.LISTENING) {
                stopDictation();
            } else if (currentState === STATE.SPEAKING || currentState === STATE.GENERATING_VOICE) {
                // If speaking, button stops playback (legacy behavior + new stop button behavior)
                if (currentAudio) {
                    currentAudio.pause();
                    currentAudio = null; // "Stop" resets it
                }
                updateState(STATE.IDLE);
            }
        };

        async function startDictation() {
            isDictating = true;
            updateState(STATE.LISTENING); // Immediate UI feedback

            // 1. Start Visualizer (User Media)
            try {
                if (!audioContext) audioContext = new (window.AudioContext || window.webkitAudioContext)();
                if (audioContext.state === 'suspended') await audioContext.resume();
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                
                microphoneStream = audioContext.createMediaStreamSource(stream);
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 32;
                microphoneStream.connect(analyser);
                dataArray = new Uint8Array(analyser.frequencyBinCount);
                drawVisualizer();
            } catch(e) {
                console.warn("Visualizer failed init", e);
            }

            // 2. Start Speech Recognition
            if (!('webkitSpeechRecognition' in window)) {
                alert("Speech Recognition not supported in this browser. Use Chrome/Edge.");
                return;
            }

            // If we already have a recognition instance, stopping might trigger onend
            if (recognition) {
                recognition.stop();
            }

            const initialText = ui.textInput.value;
            recognition = new webkitSpeechRecognition();
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';

            recognition.onresult = function(event) {
                let transcript = "";
                // Iterate over ALL results to rebuild the full transcript
                for (let i = 0; i < event.results.length; ++i) {
                    transcript += event.results[i][0].transcript;
                }

                // Append current session transcript to what was there before start
                let spacer = "";
                if (initialText && !initialText.endsWith(" ") && transcript && !transcript.startsWith(" ")) {
                    spacer = " ";
                }

                ui.textInput.value = initialText + spacer + transcript;

                // Auto-resize
                ui.textInput.style.height = 'auto';
                ui.textInput.style.height = (ui.textInput.scrollHeight) + 'px';
            };

            recognition.onerror = function(event) {
                console.warn("Speech warning", event.error);
                // Don't stop for 'no-speech' or 'aborted', just let onend restart if needed
                if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
                     isDictating = false;
                     updateState(STATE.IDLE);
                }
            };

            recognition.onend = function() {
                // Auto-restart logic
                if (isDictating) {
                    console.log("Recognition ended, restarting...");
                    try {
                        recognition.start();
                    } catch (e) {
                        console.error("Restart failed", e);
                        isDictating = false;
                        updateState(STATE.IDLE);
                    }
                }
            };

            try {
                recognition.start();
            } catch (e) {
                console.error("Start failed", e);
                isDictating = false;
                updateState(STATE.IDLE);
            }
        }

        function stopDictation() {
            isDictating = false; // Kill the auto-restart loop

            if (recognition) {
                recognition.stop();
                // recognition = null; // Keep instance for onend cleanup?
            }
            // Stop Visualizer
            if (microphoneStream) {
                microphoneStream.mediaStream.getTracks().forEach(track => track.stop());
                microphoneStream.disconnect();
                microphoneStream = null;
            }

            // Submit the text
            // Small delay to let final result process
            setTimeout(() => {
                handleTextSubmit();
            }, 500);
        }

        function b64ToBytes(b64) {
          const fixed = b64.replace(/-/g, "+").replace(/_/g, "/");
          const pad = fixed.length % 4 ? "=".repeat(4 - (fixed.length % 4)) : "";
          const bin = atob(fixed + pad);
          const bytes = new Uint8Array(bin.length);
          for (let i = 0; i < bin.length; i++) bytes[i] = bin.charCodeAt(i);
          return bytes;
        }

        window.handleInputKey = function(e) {
            if (e.key === 'Enter' && !e.shiftKey) {
                e.preventDefault(); // Prevent new line
                handleTextSubmit();
            }
        }

        window.handleTextSubmit = function() {
            const text = ui.textInput.value.trim();
            if (!text) return;
            
            ui.textInput.value = '';
            ui.textInput.style.height = 'auto'; // Reset height
            handleUserTurn({ text: text });
        }

        // Shared function for converting speech to text via Gemini
        async function transcribeWithGemini(audioBase64) {
            const url = `https://generativelanguage.googleapis.com/v1beta/models/${MODEL_ID}:generateContent?key=${API_KEY}`;
            const payload = {
                contents: [{
                    parts: [
                        { text: "Transcribe the following audio exactly as spoken. Output only the text." },
                        { inlineData: { mimeType: "audio/webm", data: audioBase64 } }
                    ]
                }]
            };
            
            const response = await fetch(url, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
            });
            
            if (!response.ok) throw new Error("Transcription failed");
            
            const data = await response.json();
            const text = data?.candidates?.[0]?.content?.parts?.[0]?.text || "";
            return text.trim();
        }

        // Handles Conversation Turn (Audio or Text)
        async function handleUserTurn(input) {
            updateState(STATE.THINKING);
            let userText = input.text;
            
            // 1. Display User Input
            addMessageToLog("You", userText);

            // 2. Send TEXT to Dr. H (Gemini) for response
            try {
                const responseText = await callGeminiAPI({ text: userText });
                addMessageToLog("Dr. H", responseText);
                await speakResponseWithGemini(responseText); // New Native Audio Function
            } catch (err) {
                console.error(err);
                updateState(STATE.IDLE);
                
                if (err.message.includes("429")) {
                    addMessageToLog("System", "Quota Limit Reached. Please wait ~1 minute before trying again.");
                } else {
                    addMessageToLog("System", "Error: " + err.message);
                }
            }
        }

        async function callGeminiAPI(input) {
            const url = `https://generativelanguage.googleapis.com/v1beta/models/${MODEL_ID}:generateContent?key=${API_KEY}`;
            
            let userPart = { text: input.text };

            // Add user turn to history
            conversationHistory.push({
                role: "user",
                parts: [userPart]
            });

            const payload = {
                contents: conversationHistory,
                // UPDATED: Dr. H Persona - Conversational Military Medical SME
                systemInstruction: {
                    parts: [{ text: "You are Dr. H, a Senior Military Medical Physician and Subject Matter Expert. Your goal is to help the provider efficiently diagnose and treat patients in a busy clinic or deployed setting.\n\nSTRUCTURE YOUR RESPONSE:\n1. **Direct Answer & DDx:** Provide a slightly more detailed, specific answer. Think critically: Do NOT blindly accept the user's premise. Proactively offer a **Differential Diagnosis (DDx)**. Explain how to **Rule In (R/I)** or **Rule Out (R/O)** specific life-threatening conditions. State the 'Most Likely Diagnosis' when appropriate based on the evidence.\n2. **Plan & Action:** Briefly outline the immediate workup (labs/imaging) or treatment required to move the patient forward.\n3. **Follow-Up:** End with **2-3 short, specific clinical questions** to narrow the differential or guide evacuation decisions. \n\nTONE: Authoritative, evidence-based, pathophysiology-focused, yet collegial. Avoid generic questions like 'What are your thoughts?'." }]
                }
            };

            const response = await fetch(url, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(payload)
            });

            if (!response.ok) {
                let details = "";
                try {
                    const j = await response.json();
                    details = j?.error?.message || JSON.stringify(j);
                } catch {
                    details = await response.text();
                }
                // Remove failed turn so retry works
                conversationHistory.pop();
                throw new Error(`API Error ${response.status}: ${details}`);
            }

            const data = await response.json();
            const text = data?.candidates?.[0]?.content?.parts
                ?.map(p => p.text)
                ?.filter(Boolean)
                ?.join("") || "";
            
            if (!text) {
                conversationHistory.pop();
                throw new Error("No response text found.");
            }
            
            // Add model response to history
            conversationHistory.push({
                role: "model",
                parts: [{ text: text }]
            });
            
            if (conversationHistory.length > 20) {
                conversationHistory = conversationHistory.slice(conversationHistory.length - 20);
            }
            
            return text;
        }

        // NEW: Generate Audio using Gemini's Aoede Voice
        async function speakResponseWithGemini(textToSpeak) {
            if (currentAudio) {
                currentAudio.pause();
                currentAudio = null;
            }
            updateState(STATE.GENERATING_VOICE);

            const url = `https://generativelanguage.googleapis.com/v1beta/models/${TTS_MODEL_ID}:generateContent?key=${API_KEY}`;
            
            const ttsPayload = {
                contents: [{ parts: [{ text: textToSpeak }] }],
                generationConfig: {
                    responseModalities: ["AUDIO"],
                    speechConfig: {
                        voiceConfig: {
                            prebuiltVoiceConfig: {
                                voiceName: "Aoede"
                            }
                        }
                    }
                }
            };

            try {
                const response = await fetch(url, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(ttsPayload)
                });

                if (!response.ok) throw new Error("TTS Generation Failed");

                const data = await response.json();
                
                // Extract audio data (Base64)
                const audioData = data.candidates?.[0]?.content?.parts?.[0]?.inlineData?.data;
                const mimeType = data.candidates?.[0]?.content?.parts?.[0]?.inlineData?.mimeType || "audio/pcm";
                
                if (!audioData) throw new Error("No audio returned");

                // Decode Safe Base64
                const bytes = b64ToBytes(audioData);

                // Convert/Blob
                let blob;
                if (mimeType.includes("pcm") || mimeType.includes("L16")) {
                  if (typeof window.pcmToWav !== "function") throw new Error("pcmToWav missing on globalThis");
                  // Int16Array view of the bytes
                  const pcm16 = new Int16Array(bytes.buffer);
                  blob = window.pcmToWav(pcm16, 1, 24000);
                } else {
                  blob = new Blob([bytes], { type: mimeType });
                }

                const audioUrl = URL.createObjectURL(blob);
                currentAudio = new Audio(audioUrl);
                currentAudio.playbackRate = 1.25;
                
                currentAudio.onended = () => {
                    updateState(STATE.IDLE);
                    // Don't null currentAudio so user can replay
                };
                
                ui.audioControls.classList.remove('hidden');
                updateState(STATE.SPEAKING);
                currentAudio.play();

            } catch (err) {
                console.error("Gemini TTS Failed, using fallback browser TTS", err);
                // Fallback to browser TTS if Gemini audio fails (e.g. quota)
                speakResponseBrowserFallback(textToSpeak); 
            }
        }

        function speakResponseBrowserFallback(text) {
            if (synthesis.speaking) synthesis.cancel();
            
            const utterance = new SpeechSynthesisUtterance(text);
            utterance.rate = 1.0;
            utterance.onend = () => updateState(STATE.IDLE);
            synthesis.speak(utterance);
        }

        function addMessageToLog(sender, text) {
            const div = document.createElement('div');
            div.className = "chat-message flex flex-col gap-1 p-3 rounded-lg max-w-[90%] shadow-sm " + 
                (sender === "You" ? "bg-slate-800 border border-slate-700 self-end ml-auto" : "bg-blue-900/20 border border-blue-500/30 self-start mr-auto");
            
            const header = document.createElement('span');
            header.className = "text-[10px] font-bold uppercase tracking-wider " + 
                (sender === "You" ? "text-slate-400" : "text-blue-400");
            header.innerText = sender;

            const content = document.createElement('p');
            content.className = "text-sm text-slate-200 leading-relaxed whitespace-pre-wrap";
            content.innerText = text;

            div.appendChild(header);
            div.appendChild(content);
            ui.conversation.appendChild(div);
            ui.conversation.scrollTop = ui.conversation.scrollHeight;
            return content; 
        }

        function drawVisualizer() {
            if (currentState !== STATE.LISTENING) {
                ui.bars.forEach(bar => bar.style.height = '4px');
                ui.barsContainer.style.opacity = '0.3';
                animationFrame = requestAnimationFrame(drawVisualizer);
                return;
            }

            ui.barsContainer.style.opacity = '1';

            if (analyser) {
                analyser.getByteFrequencyData(dataArray);
                ui.bars.forEach((bar, i) => {
                    const val = Math.max(4, (dataArray[i] / 255) * 32); 
                    bar.style.height = `${val}px`;
                });
            }
            animationFrame = requestAnimationFrame(drawVisualizer);
        }

        // Start visualizer loop immediately
        drawVisualizer();

        // --- State Management ---

        function updateState(newState) {
            currentState = newState;
            ui.statusText.innerText = newState;
            
            ui.btn.className = "w-10 h-10 rounded-full bg-slate-700 hover:bg-slate-600 flex items-center justify-center transition-colors flex-shrink-0 group";
            ui.icon.className = "material-symbols-outlined text-slate-300 group-hover:text-white transition-colors";
            ui.icon.innerText = "record_voice_over"; // Default icon

            if (newState === STATE.THINKING || newState === STATE.SPEAKING || newState === STATE.GENERATING_VOICE) {
                ui.interactionText.classList.remove('text-red-500', 'text-amber-500');
                ui.interactionText.classList.add('text-slate-500');
            }

            switch (newState) {
                case STATE.IDLE:
                    ui.statusDot.className = "w-2 h-2 rounded-full bg-slate-500";
                    if (!ui.interactionText.classList.contains('text-amber-500')) {
                        ui.interactionText.innerText = "Ready";
                    }
                    break;
                    
                case STATE.LISTENING:
                    // startVisualizer handled in startRecording
                    ui.statusDot.className = "w-2 h-2 rounded-full bg-red-500 animate-pulse";
                    ui.interactionText.innerText = "Listening...";
                    ui.icon.innerText = "stop"; 
                    ui.btn.className = "w-10 h-10 rounded-full bg-red-500 flex items-center justify-center transition-colors flex-shrink-0 animate-pulse shadow-lg shadow-red-900/50";
                    ui.icon.className = "material-symbols-outlined text-white";
                    break;
                    
                case STATE.THINKING:
                    ui.statusDot.className = "w-2 h-2 rounded-full bg-blue-500 animate-bounce";
                    ui.interactionText.innerText = "Consulting...";
                    break;

                case STATE.GENERATING_VOICE:
                    ui.statusDot.className = "w-2 h-2 rounded-full bg-indigo-500 animate-pulse";
                    ui.interactionText.innerText = "Generating Voice...";
                    break;
                    
                case STATE.SPEAKING:
                    ui.statusDot.className = "w-2 h-2 rounded-full bg-green-500 animate-pulse";
                    ui.interactionText.innerText = "Speaking...";
                    // Change button to stop/pause
                    ui.icon.innerText = "stop_circle";
                    ui.btn.className = "w-10 h-10 rounded-full bg-slate-700 hover:bg-slate-600 flex items-center justify-center transition-colors flex-shrink-0 group border border-slate-500";
                    break;
            }
        }

    </script>
</body>
</html>
